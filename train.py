# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tw1q-DYurQikob_TbnfCtaA-LgwAz6ha
"""

import argparse
import torch
import torchvision
from torchvision import datasets, transforms, models
import json
import os
from collections import OrderedDict
import time
from datetime import datetime
from torch import nn, optim

def parse_args():
    parser = argparse.ArgumentParser(description='Train a neural network')
    parser.add_argument('--data_dir', type=str, default='./flowers',
                        help='Directory containing the dataset')
    parser.add_argument('--arch', type=str, default='resnet50',
                        choices=['resnet50', 'vgg16', 'densenet121'],
                        help='Model architecture')
    parser.add_argument('--learning_rate', type=float, default=0.001,
                        help='Learning rate for the optimizer')
    parser.add_argument('--hidden_units', type=int, default=512,
                        help='Number of units in hidden layer')
    parser.add_argument('--epochs', type=int, default=20,
                        help='Number of epochs to train')
    parser.add_argument('--gpu', action='store_true',
                        help='Use GPU if available')
    parser.add_argument('--save_dir', type=str, default='./checkpoints',
                        help='Directory to save checkpoints')

    # Add this line to ignore unrecognized arguments
    args, unknown = parser.parse_known_args()
    return args

def create_model(arch, hidden_units):
    """Creates a pre-trained model with custom classifier"""
    if arch == 'resnet50':
        model = models.resnet50(pretrained=True)
        input_size = model.fc.in_features
    elif arch == 'vgg16':
        model = models.vgg16(pretrained=True)
        input_size = model.classifier[0].in_features
    elif arch == 'densenet121':
        model = models.densenet121(pretrained=True)
        input_size = model.classifier.in_features

    # Freeze parameters
    for param in model.parameters():
        param.requires_grad = False

    # Create custom classifier
    classifier = nn.Sequential(OrderedDict([
        ('fc1', nn.Linear(input_size, hidden_units)),
        ('relu', nn.ReLU()),
        ('drop', nn.Dropout(p=0.2)),
        ('fc2', nn.Linear(hidden_units, 102)),
        ('output', nn.LogSoftmax(dim=1))
    ]))

    if arch == 'resnet50':
        model.fc = classifier
    elif arch == 'vgg16':
        model.classifier = classifier
    elif arch == 'densenet121':
        model.classifier = classifier

    return model

def train(model, device, dataloaders, criterion, optimizer, epochs):
    """Trains the model"""
    print(f"\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Starting training...")

    for epoch in range(epochs):
        start_time = time.time()

        # Training loop
        model.train()
        running_loss = 0
        for inputs, labels in dataloaders['train']:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        # Validation loop
        model.eval()
        valid_loss = 0
        accuracy = 0
        with torch.no_grad():
            for inputs, labels in dataloaders['valid']:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                valid_loss += loss.item()

                _, preds = torch.max(outputs, 1)
                accuracy += (preds == labels).sum().item()

        accuracy /= len(dataloaders['valid'].dataset)

        print(f"Epoch {epoch+1}/{epochs}... "
              f"Training Loss: {running_loss/len(dataloaders['train']):.3f}... "
              f"Validation Loss: {valid_loss/len(dataloaders['valid']):.3f}... "
              f"Accuracy: {accuracy*100:.3f}%")

        print(f"[{time.time() - start_time:.2f}s]")

def save_checkpoint(model, optimizer, args, class_to_idx):
    """Saves a checkpoint"""
    checkpoint = {
        'arch': args.arch,
        'state_dict': model.state_dict(),
        'optimizer_state': optimizer.state_dict(),
        'class_to_idx': class_to_idx,
        'hidden_units': args.hidden_units,
        'learning_rate': args.learning_rate,
        'epochs': args.epochs
    }

    os.makedirs(args.save_dir, exist_ok=True)
    torch.save(checkpoint, os.path.join(args.save_dir, 'checkpoint.pth'))
    print(f"\nCheckpoint saved successfully as {os.path.join(args.save_dir, 'checkpoint.pth')}")

def main():
    # Parse arguments
    args = parse_args()

    # Set up device
    device = torch.device("cuda" if args.gpu and torch.cuda.is_available() else "cpu")
    print(f"\nUsing device: {device}")

    # Define data transforms
    data_transforms = {
        'train': transforms.Compose([
            transforms.RandomRotation(30),
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'valid': transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
    }

    # Load datasets
    image_datasets = {
        'train': datasets.ImageFolder(os.path.join(args.data_dir, 'train'), data_transforms['train']),
        'valid': datasets.ImageFolder(os.path.join(args.data_dir, 'valid'), data_transforms['valid'])
    }

    # Create dataloaders
    dataloaders = {
        'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=64, shuffle=True),
        'valid': torch.utils.data.DataLoader(image_datasets['valid'], batch_size=32)
    }

    # Create model
    model = create_model(args.arch, args.hidden_units)
    model.to(device)

    # Define loss function and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.fc.parameters(), lr=args.learning_rate)

    # Train the model
    train(model, device, dataloaders, criterion, optimizer, args.epochs)

    # Save the checkpoint
    save_checkpoint(model, optimizer, args, image_datasets['train'].class_to_idx)

if __name__ == "__main__":
    main()

